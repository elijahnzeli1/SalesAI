# -*- coding: utf-8 -*-
"""SalesAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nEp1yp07HUIa-HaKUsuI0zL46AAzzU5p
"""

# SalesA AI - Complete Multimodal AI System Implementation
# Created by: SalesA Team
# A lightweight, CPU-optimized multimodal AI with MoE architecture

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional, Union, Any
import json
import os
from pathlib import Path
import logging
from dataclasses import dataclass
import math
import random
from collections import OrderedDict
import torchvision.transforms as transforms
from PIL import Image
import torchaudio
import re
import pickle
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
from safetensors.torch import save_file # Import save_file
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import tiktoken  # Add this import at the top

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

"""# 1. CONFIGURATION AND HYPERPARAMETERS"""

@dataclass
class SalesAConfig:
    """Configuration class for SalesA AI"""
    # Model architecture
    vocab_size: int = 32000
    hidden_dim: int = 512  # Reduced for CPU efficiency
    num_layers: int = 8    # Reduced for CPU efficiency
    num_heads: int = 8
    intermediate_dim: int = 1024
    max_seq_len: int = 2048

    # MoE configuration
    num_experts: int = 4   # Reduced for CPU efficiency
    expert_capacity: int = 2
    top_k: int = 2

    # Multimodal dimensions
    vision_dim: int = 224
    audio_dim: int = 80
    vision_patch_size: int = 16
    audio_patch_size: int = 4

    # Training parameters
    batch_size: int = 4    # Small batch for CPU
    learning_rate: float = 1e-4
    weight_decay: float = 1e-5
    dropout_rate: float = 0.1

    # Optimization for CPU
    use_mixed_precision: bool = False  # CPU doesn't support AMP well
    gradient_checkpointing: bool = True

    # Data parameters
    max_text_length: int = 1024
    max_audio_length: int = 16000  # 1 second at 16kHz
    action_dim: int = 10  # Number of possible actions for robotics

    model_name: str = "SalesA AI"
    model_author: str = "Created by N.E.N (Nthuku Elijah Nzeli) and SalesA Team"

    def __post_init__(self):
        """Validate configuration"""
        assert self.hidden_dim % self.num_heads == 0, "hidden_dim must be divisible by num_heads"
        assert self.top_k <= self.num_experts, "top_k must be <= num_experts"

"""# 2. MIXTURE OF EXPERTS IMPLEMENTATION"""

class Expert(nn.Module):
    """Individual expert in the MoE layer"""
    def __init__(self, hidden_dim: int, intermediate_dim: int, dropout_rate: float = 0.1):
        super().__init__()
        self.fc1 = nn.Linear(hidden_dim, intermediate_dim)
        self.fc2 = nn.Linear(intermediate_dim, hidden_dim)
        self.dropout = nn.Dropout(dropout_rate)
        self.activation = nn.GELU()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through expert"""
        x = self.fc1(x)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x

class Router(nn.Module):
    """Router for selecting experts"""
    def __init__(self, hidden_dim: int, num_experts: int):
        super().__init__()
        self.router = nn.Linear(hidden_dim, num_experts)
        self.num_experts = num_experts

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x: Input tensor of shape (batch_size, seq_len, hidden_dim)
        Returns:
            gates: Softmax probabilities for each expert
            indices: Selected expert indices
        """
        router_logits = self.router(x)  # (batch_size, seq_len, num_experts)
        gates = F.softmax(router_logits, dim=-1)

        # Select top-k experts
        top_k_gates, top_k_indices = torch.topk(gates, k=2, dim=-1)

        return top_k_gates, top_k_indices

class MoELayer(nn.Module):
    """Mixture of Experts layer with detailed neuron-level connections"""
    def __init__(self, config: SalesAConfig):
        super().__init__()
        self.config = config
        self.num_experts = config.num_experts
        self.top_k = config.top_k

        # Create experts
        self.experts = nn.ModuleList([
            Expert(config.hidden_dim, config.intermediate_dim, config.dropout_rate)
            for _ in range(config.num_experts)
        ])

        # Router for expert selection
        self.router = Router(config.hidden_dim, config.num_experts)

        # Load balancing
        self.register_buffer('expert_usage', torch.zeros(config.num_experts))
        self.expert_usage: torch.Tensor  # type annotation for linter

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass through MoE layer

        Neuron-level connection details:
        1. Each token's hidden state goes through router
        2. Router outputs probabilities for each expert
        3. Top-k experts are selected based on probabilities
        4. Token is processed by selected experts
        5. Outputs are combined using weighted average
        """
        batch_size, seq_len, hidden_dim = x.shape

        # Get expert assignments
        gates, expert_indices = self.router(x)  # (B, S, top_k)

        # Initialize output
        output = torch.zeros_like(x)

        # Process through selected experts
        for i in range(self.top_k):
            # Get the i-th expert for each token
            expert_idx = expert_indices[:, :, i]  # (B, S)
            gate_weight = gates[:, :, i:i+1]      # (B, S, 1)

            # Process each expert
            for expert_id in range(self.num_experts):
                # Create mask for tokens assigned to this expert
                mask = (expert_idx == expert_id).unsqueeze(-1)  # (B, S, 1)

                if mask.any():
                    # Process tokens through this expert
                    expert_input = x * mask.float()
                    expert_output = self.experts[expert_id](expert_input)

                    # Add weighted contribution to output
                    output += expert_output * gate_weight * mask.float()

                    # Update usage statistics
                    self.expert_usage[expert_id] += mask.sum().item()

        return output

    def get_load_balancing_loss(self) -> torch.Tensor:
        """Calculate load balancing loss to encourage even expert usage"""
        # Normalize usage
        usage_normalized = self.expert_usage / (self.expert_usage.sum() + 1e-8)

        # Calculate coefficient of variation
        mean_usage = usage_normalized.mean()
        std_usage = usage_normalized.std()

        # Load balancing loss (encourage uniform distribution)
        load_balance_loss = (std_usage / (mean_usage + 1e-8)) ** 2

        return load_balance_loss

"""# 3. MULTIMODAL ENCODERS"""

class TextEncoder(nn.Module):
    """Text encoder with token embedding and positional encoding"""
    def __init__(self, config: SalesAConfig):
        super().__init__()
        self.config = config
        self.embedding = nn.Embedding(config.vocab_size, config.hidden_dim)
        self.positional_encoding = nn.Parameter(
            torch.randn(config.max_seq_len, config.hidden_dim)
        )
        self.dropout = nn.Dropout(config.dropout_rate)

    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:
        """Encode text tokens"""
        seq_len = input_ids.shape[1]

        # Token embeddings
        embeddings = self.embedding(input_ids)

        # Add positional encoding
        pos_encoding = self.positional_encoding[:seq_len, :].unsqueeze(0)
        embeddings = embeddings + pos_encoding

        return self.dropout(embeddings)

class VisionEncoder(nn.Module):
    """Vision encoder using patch-based approach"""
    def __init__(self, config: SalesAConfig):
        super().__init__()
        self.config = config
        self.patch_size = config.vision_patch_size
        self.num_patches = (config.vision_dim // config.vision_patch_size) ** 2

        # Patch projection
        self.patch_projection = nn.Linear(
            3 * config.vision_patch_size * config.vision_patch_size,
            config.hidden_dim
        )

        # Positional embeddings for patches
        self.pos_embedding = nn.Parameter(
            torch.randn(1, self.num_patches + 1, config.hidden_dim)
        )

        # Class token
        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_dim))

    def forward(self, images: torch.Tensor) -> torch.Tensor:
        """
        Args:
            images: (batch_size, 3, height, width)
        Returns:
            Encoded patches: (batch_size, num_patches + 1, hidden_dim)
        """
        batch_size = images.shape[0]

        # Extract patches
        patches = self.extract_patches(images)  # (B, num_patches, patch_dim)

        # Project patches
        patch_embeddings = self.patch_projection(patches)  # (B, num_patches, hidden_dim)

        # Add class token
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        embeddings = torch.cat([cls_tokens, patch_embeddings], dim=1)

        # Add positional encoding
        embeddings = embeddings + self.pos_embedding

        return embeddings

    def extract_patches(self, images: torch.Tensor) -> torch.Tensor:
        """Extract non-overlapping patches from images"""
        batch_size, channels, height, width = images.shape
        patch_size = self.patch_size

        # Unfold operation to extract patches
        patches = images.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)
        patches = patches.contiguous().view(
            batch_size, channels, -1, patch_size, patch_size
        )
        patches = patches.permute(0, 2, 1, 3, 4).contiguous()
        patches = patches.view(batch_size, -1, channels * patch_size * patch_size)

        return patches

class AudioEncoder(nn.Module):
    """Audio encoder using 1D convolutions"""
    def __init__(self, config: SalesAConfig):
        super().__init__()
        self.config = config

        # 1D convolution layers for audio processing
        self.conv1d_layers = nn.ModuleList([
            nn.Conv1d(1, 64, kernel_size=3, stride=2, padding=1),
            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.Conv1d(128, 256, kernel_size=3, stride=2, padding=1),
        ])

        # Projection to hidden dimension
        self.projection = nn.Linear(256, config.hidden_dim)

    def forward(self, audio: torch.Tensor) -> torch.Tensor:
        """
        Args:
            audio: (batch_size, seq_len) - raw audio waveform
        Returns:
            Encoded audio features: (batch_size, audio_seq_len, hidden_dim)
        """
        # Add channel dimension
        x = audio.unsqueeze(1)  # (batch_size, 1, seq_len)

        # Apply 1D convolutions
        for conv in self.conv1d_layers:
            x = F.relu(conv(x))

        # Transpose for sequence modeling
        x = x.transpose(1, 2)  # (batch_size, seq_len, channels)

        # Project to hidden dimension
        x = self.projection(x)

        return x

"""# 4. TRANSFORMER ARCHITECTURE"""

class MultiHeadAttention(nn.Module):
    """Multi-head self-attention mechanism"""
    def __init__(self, config: SalesAConfig):
        super().__init__()
        self.config = config
        self.hidden_dim = config.hidden_dim
        self.num_heads = config.num_heads
        self.head_dim = config.hidden_dim // config.num_heads

        self.query = nn.Linear(config.hidden_dim, config.hidden_dim)
        self.key = nn.Linear(config.hidden_dim, config.hidden_dim)
        self.value = nn.Linear(config.hidden_dim, config.hidden_dim)
        self.output = nn.Linear(config.hidden_dim, config.hidden_dim)

        self.dropout = nn.Dropout(config.dropout_rate)

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size, seq_len, _ = x.shape

        # Linear projections
        q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # Scaled dot-product attention
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)

        # Apply mask if provided
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Softmax and dropout
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        # Apply attention to values
        context = torch.matmul(attention_weights, v)

        # Concatenate heads
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.hidden_dim
        )

        # Output projection
        output = self.output(context)

        return output

class TransformerBlock(nn.Module):
    """Transformer block with MoE integration"""
    def __init__(self, config: SalesAConfig):
        super().__init__()
        self.config = config

        # Multi-head attention
        self.attention = MultiHeadAttention(config)
        self.attention_norm = nn.LayerNorm(config.hidden_dim)

        # MoE layer instead of standard FFN
        self.moe: MoELayer = MoELayer(config)
        self.moe_norm = nn.LayerNorm(config.hidden_dim)

        self.dropout = nn.Dropout(config.dropout_rate)

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # Multi-head attention with residual connection
        attention_output = self.attention(x, mask)
        x = self.attention_norm(x + self.dropout(attention_output))

        # MoE with residual connection
        moe_output = self.moe(x)
        x = self.moe_norm(x + self.dropout(moe_output))

        return x

"""# 5. MAIN SALESA AI MODEL:"""

class SalesAModel(nn.Module):
    """Main SalesA AI model with multimodal capabilities"""
    def __init__(self, config: SalesAConfig):
        super().__init__()
        self.config = config
        self.model_name = config.model_name
        self.model_author = config.model_author

        # Multimodal encoders
        self.text_encoder = TextEncoder(config)
        self.vision_encoder = VisionEncoder(config)
        self.audio_encoder = AudioEncoder(config)

        # Transformer layers
        self.transformer_blocks = nn.ModuleList([
            TransformerBlock(config) for _ in range(config.num_layers)
        ])

        # Output heads for different tasks
        self.text_head = nn.Linear(config.hidden_dim, config.vocab_size)
        self.vision_head = nn.Linear(config.hidden_dim, config.vocab_size)  # For vision-to-text
        self.audio_head = nn.Linear(config.hidden_dim, config.vocab_size)   # For audio-to-text
        self.code_head = nn.Linear(config.hidden_dim, config.vocab_size)
        self.action_head = nn.Linear(config.hidden_dim, config.action_dim)  # For robotics actions

        # TTS integration (placeholder, can be replaced with real TTS model)
        self.tts = None  # Placeholder for TTS module

        # Initialize weights
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """Initialize weights using Xavier/Glorot initialization"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.ones_(module.weight)
            torch.nn.init.zeros_(module.bias)

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                images: Optional[torch.Tensor] = None,
                audio: Optional[torch.Tensor] = None,
                task_type: str = "text",
                return_loss: bool = False,
                labels: Optional[torch.Tensor] = None,
                action_labels: Optional[torch.Tensor] = None) -> Dict[str, Any]:
        """
        Forward pass through SalesA AI

        Args:
            input_ids: Text token IDs
            images: Image tensors
            audio: Audio waveforms
            task_type: Type of task ("text", "vision", "audio", "code", "action")
            return_loss: Whether to compute loss
            labels: Ground truth labels for loss computation
            action_labels: Action labels for robotics tasks
        """
        # Encode inputs based on modality
        embeddings = []

        if input_ids is not None:
            text_embeddings = self.text_encoder(input_ids)
            embeddings.append(text_embeddings)

        if images is not None:
            vision_embeddings = self.vision_encoder(images)
            embeddings.append(vision_embeddings)

        if audio is not None:
            audio_embeddings = self.audio_encoder(audio)
            embeddings.append(audio_embeddings)

        # Concatenate all embeddings
        if len(embeddings) == 1:
            x = embeddings[0]
        else:
            x = torch.cat(embeddings, dim=1)

        # Pass through transformer blocks
        for block in self.transformer_blocks:
            x = block(x)

        # Generate outputs based on task type
        if task_type == "text":
            logits = self.text_head(x)
        elif task_type == "vision":
            logits = self.vision_head(x)
        elif task_type == "audio":
            logits = self.audio_head(x)
        elif task_type == "code":
            logits = self.code_head(x)
        elif task_type == "action":
            logits = self.action_head(x)
        else:
            raise ValueError(f"Unknown task type: {task_type}")

        # Calculate loss if requested
        loss = None
        if return_loss:
            if task_type == "action" and action_labels is not None:
                # For discrete actions, use cross-entropy
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), action_labels.view(-1))
            elif labels is not None:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))

        return {
            "logits": logits,
            "loss": loss,
            "hidden_states": x
        }

    def tts_generate(self, text: str) -> bytes:
        """Text-to-speech: returns waveform bytes (placeholder, replace with real TTS)"""
        # Placeholder: return silence or use torchaudio TTS if available
        # You can integrate a real TTS model here (e.g., TTS from Coqui, torchaudio pipelines, etc.)
        import numpy as np
        sr = 22050
        duration = 2  # seconds
        waveform = np.zeros(int(sr * duration), dtype=np.float32)  # Silence
        return waveform.tobytes()

    def generate(self,
                 input_ids: torch.Tensor,
                 max_length: int = 100,
                 temperature: float = 0.7,
                 do_sample: bool = True,
                 top_k: int = 50) -> torch.Tensor:
        """Generate text using the model"""
        self.eval()
        generated = input_ids.clone()

        with torch.no_grad():
            for _ in range(max_length):
                # Get model predictions
                outputs = self.forward(generated, task_type="text")
                logits = outputs["logits"][:, -1, :]  # Get last token predictions

                # Apply temperature
                logits = logits / temperature

                # Sample next token
                if do_sample:
                    # Top-k sampling
                    if top_k > 0:
                        top_k_logits, top_k_indices = torch.topk(logits, top_k)
                        probs = F.softmax(top_k_logits, dim=-1)
                        next_token_idx = torch.multinomial(probs, 1)
                        next_token = top_k_indices.gather(1, next_token_idx)
                    else:
                        probs = F.softmax(logits, dim=-1)
                        next_token = torch.multinomial(probs, 1)
                else:
                    # Greedy decoding
                    next_token = torch.argmax(logits, dim=-1, keepdim=True)

                # Append to generated sequence
                generated = torch.cat([generated, next_token], dim=1)

                # Stop if we generate end token (assuming 2 is end token)
                if next_token.item() == 2:
                    break

        return generated

    def get_name(self):
        return self.model_name

    def get_author(self):
        return self.model_author

"""# 6. DATASET CLASSES"""

# --- Define extra domain-specific tokens ---
EXTRA_TOKENS = [
    "<STOCK>", "<FINANCE>", "<MARKET>", "<SALES>", "<EARNINGS>", "<REVENUE>", "<PRICE>", "<VOLUME>",
    "<ACTION>", "<AUDIO>", "<VISION>", "<TTS>", "<ROBOT>", "<INSTRUCTION>", "<RESPONSE>", "<USER>", "<ASSISTANT>"
]

class SalesATokenizer:
    """Tokenizer for SalesA AI using Tiktoken and extra domain tokens"""
    def __init__(self, vocab_size: int = 32000, vocab: Optional[list] = None, enc=None, model_name: str = "gpt2"):
        self.vocab_size = vocab_size
        if enc is not None:
            self.enc = enc
        else:
            self.enc = tiktoken.get_encoding(model_name)
        if vocab is not None:
            self.vocab = vocab[:vocab_size]
        else:
            self.vocab = list(self.enc._mergeable_ranks.keys())[:vocab_size]
        self.token_to_id = {token: i for i, token in enumerate(self.vocab)}
        self.id_to_token = {i: token for i, token in enumerate(self.vocab)}
        # Special tokens
        self.pad_token = "<|pad|>"
        self.unk_token = "<|unk|>"
        self.bos_token = "<|startoftext|>"
        self.eos_token = "<|endoftext|>"
        self.code_token = "<|code|>"
        # Add extra tokens to token_to_id if not present
        for token in EXTRA_TOKENS:
            token_bytes = token.encode("utf-8") if isinstance(token, str) else token
            if token_bytes not in self.token_to_id:
                idx = len(self.token_to_id)
                self.token_to_id[token_bytes] = idx
                self.id_to_token[idx] = token_bytes
        # Set special token IDs safely
        try:
            self.pad_token_id = self.enc.encode(self.pad_token)[0]
        except Exception:
            self.pad_token_id = 0
        try:
            self.unk_token_id = self.enc.encode(self.unk_token)[0]
        except Exception:
            self.unk_token_id = 1
        try:
            self.bos_token_id = self.enc.encode(self.bos_token)[0]
        except Exception:
            self.bos_token_id = 2
        try:
            self.eos_token_id = self.enc.encode(self.eos_token)[0]
        except Exception:
            self.eos_token_id = 3
        try:
            self.code_token_id = self.enc.encode(self.code_token)[0]
        except Exception:
            self.code_token_id = 4
        # Add extra token IDs
        for token in EXTRA_TOKENS:
            token_bytes = token.encode("utf-8") if isinstance(token, str) else token
            setattr(self, f"{token.strip('<>').lower()}_id", self.token_to_id[token_bytes])
    def encode(self, text: str) -> list:
        # Use Tiktoken for base encoding, but map extra tokens to their IDs
        tokens = []
        for word in text.split():
            word_bytes = word.encode("utf-8")
            if word_bytes in self.token_to_id:
                tokens.append(self.token_to_id[word_bytes])
            else:
                tokens.extend(self.enc.encode(word))
        return tokens
    def decode(self, token_ids: list) -> str:
        # Map extra token IDs back to their string, otherwise use Tiktoken
        words = []
        for tid in token_ids:
            if tid in self.id_to_token and self.id_to_token[tid].decode("utf-8") in EXTRA_TOKENS:
                words.append(self.id_to_token[tid].decode("utf-8"))
            else:
                try:
                    words.append(self.enc.decode([tid]))
                except Exception:
                    words.append("<UNK>")
        return " ".join(words)

from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn.functional as F
import random
import logging
from typing import Dict, List, Optional, Union
import torchvision.transforms as transforms
from PIL import Image
import torchaudio
from datasets import load_dataset
import numpy as np
import io
import soundfile as sf  # Added for audio decoding

logger = logging.getLogger(__name__)

class MultimodalDataset(Dataset):
    """Dataset for multimodal training with real datasets and action support"""

    def __init__(self, config, tokenizer: SalesATokenizer, split: str = "train", dataset_name: Union[str, List[str]] = "auto", task_type: Optional[str] = None):
        self.config = config
        self.tokenizer = tokenizer
        self.split = split
        self.dataset_name = dataset_name
        self.task_type = task_type
        self.data = self._load_data()   

    def _load_data(self) -> List[Dict]:
        """Load and prepare multimodal data from real datasets"""
        data = []

        # Define available datasets with their configurations
        dataset_configs = {
            # Audio–text captioning
            "clotho": {
                "name": "confit/clotho",
                "config": None,
                "has_image": False,
                "has_text": True,
                "has_audio": True,
                "audio_key": "audio",
                "text_key": "captions",
                "limit": 2000 if self.split == "train" else 500
            },
            # Audio–text question answering
            "clotho_aqa": {
                "name": "CLAPv2/ClothoAQA",
                "config": None,
                "has_image": False,
                "has_text": True,
                "has_audio": True,
                "audio_key": "audio",
                "text_key": "question",
                "answer_key": "answer",
                "limit": 1500 if self.split == "train" else 500
            },
            # Tri‑modal: image + audio + text
            "luma": {
                "name": "bezirganyan/LUMA",
                "config": None,
                "has_image": True,
                "has_text": True,
                "has_audio": True,
                "image_key": "image",
                "audio_key": "audio",
                "text_key": "text",
                "limit": 2000 if self.split == "train" else 500
            },
            # Beans you already have:
            "beans": {
                "name": "AI-Lab-Makerere/beans",
                "config": None,
                "has_image": True,
                "has_text": False,
                "has_audio": False,
                "image_key": "image",
                "text_key": None,
                "limit": 1000 if self.split == "train" else 200
            },
            "prosocial_dialog": {
                "name": "allenai/prosocial-dialog",
                "config": None,
                "has_image": False,
                "has_text": True,
                "has_audio": False,
                "text_key": "text",
                "labels_key": "labels",
                "answer_key": None,
                "limit": 50000 if self.split == "train" else 10000
            },
            "logic_reasoning": {
                "name": "LogiQA",
                "config": None,
                "has_text": True,
                "task": "multiple_choice",
                "limit": 8000 if self.split=="train" else 2000
            },
            # Added garage-bAInd/Open-Platypus dataset
            "open_platypus": {
                "name": "garage-bAInd/Open-Platypus",
                "config": None, # Assuming default config
                "has_image": False,
                "has_text": True,
                "has_audio": False,
                "text_key": "text", # Common key for text in this type of dataset
                "limit": 10000 if self.split == "train" else 2000 # Example limit
            },
            # Removed problematic datasets for now
            # Financial PhraseBank (default for financial/stock tasks)
            "financial_phrasebank": {
                "name": "atrost/financial_phrasebank",
                "config": "sentences_50agree",
                "has_image": False,
                "has_text": True,
                "has_audio": False,
                "text_key": "sentence",
                "labels_key": "label",
                "limit": 5000 if self.split == "train" else 1000
            },
            # Code generation/understanding datasets
            "humaneval": {
                "name": "code-rag-bench/humaneval",
                "config": None,
                "has_image": False,
                "has_text": True,
                "has_audio": False,
                "text_key": "prompt",  # Humaneval uses 'prompt' for code tasks
                "labels_key": "canonical_solution",  # Ground truth code
                "limit": 1000 if self.split == "train" else 200
            },
            "ds1000": {
                "name": "code-rag-bench/ds1000",
                "config": None,
                "has_image": False,
                "has_text": True,
                "has_audio": False,
                "text_key": "prompt",  # DS1000 uses 'prompt' for code tasks
                "labels_key": "solution",  # Ground truth code
                "limit": 1000 if self.split == "train" else 200
            },
        }

        # --- Enhanced automatic selection logic ---
        if self.dataset_name == "auto":
            # Select default dataset(s) based on task_type
            if hasattr(self, 'task_type') and self.task_type:
                if self.task_type in ["code", "code-generation"]:
                    selected_datasets = ["humaneval", "ds1000"]
                elif self.task_type == "vision":
                    selected_datasets = ["beans"]
                elif self.task_type == "audio":
                    selected_datasets = ["clotho"]
                elif self.task_type in ["financial", "stock"]:
                    selected_datasets = ["financial_phrasebank"]
                elif self.task_type == "text":
                    selected_datasets = ["prosocial_dialog"]
                else:
                    selected_datasets = ["logic_reasoning"]
            else:
                # Fallback to general text dataset
                selected_datasets = ["open_platypus"]
        elif self.dataset_name == "all":
            selected_datasets = list(dataset_configs.keys())
        elif isinstance(self.dataset_name, list):
            selected_datasets = self.dataset_name
        else:
            selected_datasets = [self.dataset_name]

        # Load each selected dataset
        for dataset_key in selected_datasets:
            if dataset_key not in dataset_configs:
                logger.warning(f"Unknown dataset: {dataset_key}")
                continue

            dataset_config = dataset_configs[dataset_key]
            try:
                # Attempt to load the dataset
                loaded_data = self._load_single_dataset(dataset_config)
                data.extend(loaded_data)
            except Exception as e:
                logger.error(f"Failed to load dataset {dataset_key}: {e}")
                # Continue to the next dataset even if one fails

        # If no real data was loaded, fall back to synthetic data
        if not data:
            logger.warning("No real datasets loaded successfully. Generating synthetic data.")
            data = self._generate_synthetic_data()

        logger.info(f"Total samples loaded: {len(data)}")
        return data

    def _load_single_dataset(self, dataset_config: Dict) -> List[Dict]:
        """Load a single dataset based on its configuration"""
        data = []

        logger.info(f"Loading dataset: {dataset_config['name']}")

        try:
            # Handle special split cases
            if dataset_config['name'] == "bezirganyan/LUMA" and self.split == "validation":
                logger.warning("LUMA doesn't have validation split. Using test split instead.")
                split_to_use = "test"
            elif dataset_config['name'] == "garage-bAInd/Open-Platypus" and self.split == "validation":
                 # Suppress the warning here as we are handling it
                 split_to_use = "train"
            else:
                split_to_use = self.split


            # Load dataset
            if dataset_config['config']:
                dataset = load_dataset(
                    dataset_config['name'],
                    dataset_config['config'],
                    split=split_to_use,
                    streaming=False  # Disable streaming for reliability
                )
            else:
                dataset = load_dataset(
                    dataset_config['name'],
                    split=split_to_use,
                    streaming=False  # Disable streaming for reliability
                )

            # Process samples
            count = 0
            for item in dataset:
                if count >= dataset_config['limit']:
                    break
                if not isinstance(item, dict):
                    continue
                try:
                    sample = self._process_sample(item, dataset_config)
                    if sample:
                        data.append(sample)
                        count += 1

                        if count % 1000 == 0:
                            logger.info(f"Processed {count} samples from {dataset_config['name']}")

                except Exception as e:
                    logger.warning(f"Error processing sample {count} from {dataset_config['name']}: {e}")
                    continue

        except Exception as e:
            logger.error(f"Error loading dataset {dataset_config['name']}: {e}")
            # Generate synthetic data for this dataset instead of failing completely
            logger.info(f"Generating synthetic data for {dataset_config['name']}")
            synthetic_data = self._generate_synthetic_data_for_dataset(dataset_config)
            data.extend(synthetic_data)

        logger.info(f"Successfully loaded {len(data)} samples from {dataset_config['name']}")
        return data

    def _generate_synthetic_data_for_dataset(self, dataset_config: Dict) -> List[Dict]:
        """Generate synthetic data for a specific dataset configuration"""
        num_samples = dataset_config['limit']
        synthetic_data = []

        logger.info(f"Generating {num_samples} synthetic samples for {dataset_config['name']}")

        for i in range(num_samples):
            # Create sample based on dataset configuration
            sample = {
                "text": None,
                "image": None,
                "audio": None,
                "task_type": "text",
                "labels": None,
                "action_labels": None  # For robotics
            }

            # Generate text if needed
            if dataset_config['has_text']:
                text_length = random.randint(10, 50)
                text_tokens = torch.randint(1, self.config.vocab_size, (text_length,))
                sample["text"] = text_tokens
                sample["labels"] = text_tokens.clone()

            # Generate image if needed
            if dataset_config['has_image']:
                image = torch.randn(3, self.config.vision_dim, self.config.vision_dim)
                image = torch.clamp(image, -2, 2)
                sample["image"] = image
                # --- FIX: Always generate a valid label for vision samples ---
                sample["labels"] = torch.tensor([random.randint(0, 9)], dtype=torch.long)  # 10-class synthetic label

            # Generate audio if needed
            if dataset_config['has_audio']:
                audio = torch.randn(self.config.max_audio_length) * 0.1
                sample["audio"] = audio

            # Generate action label for robotics
            if random.random() < 0.2:  # 20% of samples are action samples
                sample["action_labels"] = torch.tensor([random.randint(0, self.config.action_dim - 1)], dtype=torch.long)
                sample["task_type"] = "action"

            # Determine task type
            if sample["image"] is not None and sample["text"] is not None:
                sample["task_type"] = "vision_text"
            elif sample["audio"] is not None and sample["text"] is not None:
                sample["task_type"] = "audio_text"
            elif sample["text"] is not None:
                sample["task_type"] = "text"
            elif sample["image"] is not None:
                sample["task_type"] = "vision"
            elif sample["audio"] is not None:
                sample["task_type"] = "audio"

            synthetic_data.append(sample)

        return synthetic_data

    def _process_sample(self, item: Dict, dataset_config: Dict) -> Optional[Dict]:
        """Process a single sample from a dataset"""

        # Initialize sample
        sample = {
            "text": None,
            "image": None,
            "audio": None,
            "task_type": "text",
            "labels": None
        }

        # Process text
        if dataset_config['has_text'] and dataset_config['text_key'] in item:
            text_content = item[dataset_config['text_key']]
            # Prepend <STOCK> token for financial_phrasebank
            if dataset_config.get('name', '') == 'financial_phrasebank':
                text_content = "<STOCK> " + text_content
            # Handle different text formats
            if isinstance(text_content, list):
                # For datasets with multiple captions
                text_content = random.choice(text_content)
            if isinstance(text_content, dict):
                text_content = text_content.get('raw', str(text_content))
            if text_content and isinstance(text_content, str) and len(text_content.strip()) > 0:
                try:
                    text_tokens = torch.tensor(self.tokenizer.encode(text_content), dtype=torch.long)
                    sample["text"] = text_tokens
                    sample["labels"] = text_tokens.clone()
                except Exception as e:
                    logger.warning(f"Error tokenizing text: {e}")
            # For financial_phrasebank, set sentiment label as classification label
            if dataset_config.get('name', '') == 'financial_phrasebank' and 'labels_key' in dataset_config and dataset_config['labels_key'] in item:
                sample["labels"] = torch.tensor([int(item[dataset_config['labels_key']])], dtype=torch.long)

        # Process images
        if dataset_config['has_image'] and dataset_config['image_key'] in item:
            try:
                image_data = item[dataset_config['image_key']]

                # Handle different image formats
                if isinstance(image_data, str):
                    # Skip URL images for now
                    logger.warning(f"Skipping image with URL: {image_data}")
                elif hasattr(image_data, 'convert'):
                    # PIL Image
                    img = image_data.convert("RGB")
                    transform = transforms.Compose([
                        transforms.Resize((self.config.vision_dim, self.config.vision_dim)),
                        transforms.ToTensor(),
                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                    ])
                    sample["image"] = transform(img)
                elif isinstance(image_data, np.ndarray):
                    # Array format
                    img = Image.fromarray(image_data).convert("RGB")
                    transform = transforms.Compose([
                        transforms.Resize((self.config.vision_dim, self.config.vision_dim)),
                        transforms.ToTensor(),
                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
                    ])
                    sample["image"] = transform(img)
                else:
                    logger.warning(f"Unsupported image format: {type(image_data)}")

            except Exception as e:
                logger.warning(f"Error processing image: {e}")

        # Process audio - using soundfile as a fallback
        if dataset_config['has_audio'] and dataset_config['audio_key'] in item:
            try:
                audio_data = item[dataset_config['audio_key']]
                waveform = None
                sample_rate = 16000

                # Handle different audio formats
                if isinstance(audio_data, dict) and 'array' in audio_data:
                    # Standard audio dictionary format
                    waveform = torch.tensor(audio_data['array'], dtype=torch.float32)
                    sample_rate = audio_data.get('sampling_rate', 16000)
                elif isinstance(audio_data, bytes):
                    # Audio bytes
                    with io.BytesIO(audio_data) as f:
                        data, sample_rate = sf.read(f)
                        waveform = torch.tensor(data, dtype=torch.float32)
                elif isinstance(audio_data, str):
                    # File path - skip for now
                    logger.warning(f"Skipping audio file path: {audio_data}")
                elif isinstance(audio_data, np.ndarray):
                    # Numpy array
                    waveform = torch.tensor(audio_data, dtype=torch.float32)

                if waveform is not None:
                    # Resample if necessary
                    if sample_rate != 16000:
                        resampler = torchaudio.transforms.Resample(
                            orig_freq=sample_rate,
                            new_freq=16000
                        )
                        waveform = resampler(waveform)

                    # Ensure mono
                    if waveform.dim() > 1:
                        waveform = waveform.mean(dim=0)

                    # Pad or truncate
                    if len(waveform) > self.config.max_audio_length:
                        waveform = waveform[:self.config.max_audio_length]
                    else:
                        waveform = F.pad(waveform, (0, self.config.max_audio_length - len(waveform)))

                    sample["audio"] = waveform

            except Exception as e:
                logger.warning(f"Error processing audio: {e}")

        # --- FIX: For vision-only samples, ensure a valid label is present ---
        if dataset_config.get('has_image', False) and not dataset_config.get('has_text', False):
            # Try to get a label for image classification
            label = item.get('label') or item.get('labels')
            if label is not None:
                # If label is a string, map to int or use tokenizer if vision-to-text
                if isinstance(label, str):
                    # Use tokenizer if vision-to-text, else hash to int
                    label_id = self.tokenizer.encode(label)[0] if hasattr(self, 'tokenizer') else abs(hash(label)) % 1000
                else:
                    label_id = int(label)
                sample["labels"] = torch.tensor([label_id], dtype=torch.long)
            else:
                # If no label, skip this sample
                return None

        # Handle VQA specific format
        if 'answer_key' in dataset_config and dataset_config['answer_key'] in item:
            answer = item[dataset_config['answer_key']]
            if sample["text"] is not None:
                try:
                    question_text = self.tokenizer.decode(sample["text"].tolist())
                    combined_text = f"Question: {question_text} Answer: {answer}"
                    combined_tokens = torch.tensor(self.tokenizer.encode(combined_text), dtype=torch.long)
                    sample["labels"] = combined_tokens
                except:
                    pass

        # Determine the primary task type
        if sample["image"] is not None and sample["text"] is not None:
            sample["task_type"] = "vision_text"
        elif sample["audio"] is not None and sample["text"] is not None:
            sample["task_type"] = "audio_text"
        elif sample["text"] is not None:
            sample["task_type"] = "text"
        elif sample["image"] is not None:
            sample["task_type"] = "vision"
        elif sample["audio"] is not None:
            sample["task_type"] = "audio"
        else:
            # If no valid modality data, skip
            return None

        # Ensure labels are set
        if sample["task_type"] in ["text", "vision_text", "audio_text"] and sample["labels"] is None:
            if sample["text"] is not None:
                sample["labels"] = sample["text"].clone()
            else:
                return None


        return sample

    def _generate_synthetic_data(self) -> List[Dict]:
        """Generate synthetic data as fallback"""
        logger.info("Generating synthetic multimodal data")

        data = []
        num_samples = 1000 if self.split == "train" else 200

        for i in range(num_samples):
            text_length = random.randint(10, 50)
            text_tokens = torch.randint(1, self.config.vocab_size, (text_length,))

            # Generate realistic image tensors
            image = torch.randn(3, self.config.vision_dim, self.config.vision_dim)
            image = torch.clamp(image, -2, 2)  # Normalize range

            # Generate realistic audio
            audio = torch.randn(self.config.max_audio_length) * 0.1

            task_type = random.choice(["text", "vision", "audio", "vision_text", "audio_text"])
            labels = text_tokens.clone() if task_type in ["text", "vision_text", "audio_text"] else None

            sample = {
                "text": text_tokens if task_type in ["text", "vision_text", "audio_text"] else None,
                "image": image if task_type in ["vision", "vision_text"] else None,
                "audio": audio if task_type in ["audio", "audio_text"] else None,
                "task_type": task_type,
                "labels": labels
            }

            # Ensure at least one modality is present
            if sample["text"] is not None or sample["image"] is not None or sample["audio"] is not None:
                data.append(sample)

        return data


    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

def collate_fn(batch):
    """Enhanced collate function for multimodal data with action support"""
    if not batch:
        return {
            "input_ids": torch.zeros(0, dtype=torch.long),
            "attention_mask": torch.zeros(0, dtype=torch.long),
            "images": torch.zeros(0, 3, 224, 224),
            "audio": torch.zeros(0, 16000),
            "task_types": [],
            "labels": torch.zeros(0, dtype=torch.long),
            "action_labels": torch.zeros(0, dtype=torch.long)
        }

    # Separate different modalities
    texts = [item["text"] for item in batch if item.get("text") is not None]
    images = [item["image"] for item in batch if item.get("image") is not None]
    audios = [item["audio"] for item in batch if item.get("audio") is not None]
    task_types = [item["task_type"] for item in batch]
    labels = [item["labels"] for item in batch if item.get("labels") is not None]
    action_labels = [item["action_labels"] for item in batch if item.get("action_labels") is not None]


    # Find max lengths for padding (only for present modalities)
    max_text_len = max(len(text) for text in texts) if texts else 0
    # Assuming fixed size for images and audio based on config
    max_image_tokens = (images[0].shape[1] // 16) * (images[0].shape[2] // 16) + 1 if images else 0
    max_audio_tokens = 100 if audios else 0 # Simplified length calculation for audio

    # Determine the total sequence length for padding
    # Ensure consistent sequence length across the batch for padding attention mask and labels
    # This requires padding each modality sequence up to its max length *within the batch*
    # and then concatenating the padded sequences.

    # We need to determine the maximum total sequence length including all modalities
    # For each sample, calculate its potential total length
    sample_lengths = []
    for item in batch:
        current_length = 0
        if item.get("text") is not None:
             current_length += len(item["text"])
        if item.get("image") is not None:
             # Use the expected number of vision tokens
             current_length += (item["image"].shape[1] // 16) * (item["image"].shape[2] // 16) + 1
        if item.get("audio") is not None:
            # Use a simplified audio token length or a calculated one based on convolution output shape
            current_length += 100 # This should ideally be calculated based on the audio encoder output
        sample_lengths.append(current_length)

    final_sequence_length = max(sample_lengths) if sample_lengths else 0


    # Initialize tensors
    batch_size = len(batch)
    # Use None for modalities not present in the batch
    texts_tensor = torch.zeros(batch_size, max_text_len, dtype=torch.long) if max_text_len > 0 else None
    images_tensor = torch.stack(images) if images else None
    audios_tensor = torch.stack(audios) if audios else None

    # Labels tensor size should match the final sequence length
    labels_tensor = torch.full((batch_size, final_sequence_length), -100, dtype=torch.long)
    attention_mask = torch.zeros(batch_size, final_sequence_length, dtype=torch.long)

    # Add action_labels tensor
    action_labels_tensor = torch.full((batch_size, 1), -100, dtype=torch.long)
    for i, item in enumerate(batch):
        if item.get("action_labels") is not None:
            action_labels_tensor[i, 0] = item["action_labels"][0]

    # Process each item individually for padding and mask creation
    for i, item in enumerate(batch):
        current_pos = 0

        # Process text
        if item.get("text") is not None:
            text = item["text"]
            length = len(text)
            if texts_tensor is not None:
                texts_tensor[i, :length] = text
            attention_mask[i, current_pos : current_pos + length] = 1
            if item.get("labels") is not None:
                labels_tensor[i, current_pos : current_pos + length] = item["labels"][:length] # Ensure label length matches text length
            current_pos += max_text_len # Move current_pos by the maximum text length in the batch

        # Process images
        if item.get("image") is not None:
            img_tokens_len = (item["image"].shape[1] // 16) * (item["image"].shape[2] // 16) + 1
            attention_mask[i, current_pos : current_pos + img_tokens_len] = 1
            # For vision-to-text or multimodal tasks, labels might align with text output.
            # If the task is purely vision-based or the labels are associated with the vision part,
            # you'll need to adjust the label placement here. Assuming text-aligned labels for now.
            current_pos += max_image_tokens # Move current_pos by the maximum image token length

        # Process audio
        if item.get("audio") is not None:
             audio_tokens_len = 100 # This should be derived from the audio encoder output shape
             attention_mask[i, current_pos : current_pos + audio_tokens_len] = 1
             # Similar to images, adjust label placement if needed.
             current_pos += max_audio_tokens # Move current_pos by the maximum audio token length


    return {
        "input_ids": texts_tensor,
        "attention_mask": attention_mask,
        "images": images_tensor,
        "audio": audios_tensor,
        "task_types": task_types,
        "labels": labels_tensor,
        "action_labels": action_labels_tensor
    }


# Example usage functions
def create_multimodal_dataloaders(config, tokenizer, batch_size=8, dataset_name="auto"):
    """Create train and validation dataloaders"""
    # Create datasets
    train_dataset = MultimodalDataset(
        config=config,
        tokenizer=tokenizer,
        split="train",
        dataset_name=dataset_name
    )

    val_dataset = MultimodalDataset(
        config=config,
        tokenizer=tokenizer,
        split="validation" if dataset_name != "luma" else "test",  # Handle LUMA special case
        dataset_name=dataset_name
    )

    # Create dataloaders
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=2,
        pin_memory=True
    )

    val_dataloader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=2,
        pin_memory=True
    )

    return train_dataloader, val_dataloader

# Available datasets information
AVAILABLE_DATASETS = {
    "prosocial_dialog": "Text-only multi-turn conversations with prosocial responses (58K dialogues)",
    "beans": "Image + Labels (Beans leaf disease)",
    "clotho": "Audio + Captions (~5k audio samples)",
    "clotho_aqa": "Audio + Q&A (~2k samples)",
    "luma": "Image + Audio + Text (~50 classes multimodal)",
    "logic_reasoning": "Text-only reasoning multi-turn conversations with prosocial responses (58K dialogues)",
    "open_platypus": "Text-only instruction following dataset", # Added Open-Platypus description
    "humaneval": "Code generation/understanding: HumanEval benchmark (code prompts and solutions)",
    "ds1000": "Code generation/understanding: DS-1000 benchmark (code prompts and solutions)",
}


def print_dataset_info():
    """Print information about available datasets"""
    print("Available datasets:")
    for name, description in AVAILABLE_DATASETS.items():
        print(f"  - {name}: {description}")
    print("\nRecommended stable datasets: 'beans', 'prosocial_dialog', 'open_platypus'") # Updated recommendation
    print("Audio datasets require additional dependencies: pip install soundfile")

"""# 8. TRAINING LOOP"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional, Union
import json
import os
from pathlib import Path
import logging
from dataclasses import dataclass
import math
import random
from collections import OrderedDict
import torchvision.transforms as transforms
from PIL import Image
import torchaudio
import re
import pickle
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
from safetensors.torch import save_file # Import save_file

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SalesATrainer:
    """Training class for SalesA AI"""
    def __init__(self, config: SalesAConfig):
        super().__init__()
        self.config = config
        self.model = SalesAModel(config)
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        # Use a placeholder scheduler for now; warmup will be handled manually
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, T_max=1000 # Placeholder T_max, adjusted by warmup
        )

        # Training metrics
        self.train_losses = []
        self.val_losses = []
        self.expert_usage_history = []
        self.gradient_norm_history = [] # Added for monitoring gradient norms

        # Gradient accumulation setup
        self.gradient_accumulation_steps = 8 # Accumulate gradients over 8 batches
        self.total_training_steps = 0 # To track total steps for warmup

    def train_epoch(self, train_loader: DataLoader) -> float:
        """Train for one epoch"""
        self.model.train()
        total_loss = 0
        total_load_balance_loss = 0
        epoch_gradient_norms = [] # To store gradient norms for this epoch

        progress_bar = tqdm(train_loader, desc="Training")

        for batch_idx, batch in enumerate(progress_bar):
            # Apply learning rate warmup
            self.total_training_steps += 1
            self._apply_warmup_lr(self.total_training_steps, 1000) # Warmup over first 1000 steps

            # Forward pass for each task type in batch
            batch_loss = 0
            batch_load_balance_loss = 0
            valid_samples_in_batch = 0

            for i, task_type in enumerate(batch["task_types"]):
                # Get single item from batch, handle None cases
                input_ids = batch["input_ids"][i:i+1] if batch["input_ids"] is not None and batch["input_ids"].numel() > 0 else None
                images = batch["images"][i:i+1] if batch["images"] is not None and batch["images"].numel() > 0 else None
                audio = batch["audio"][i:i+1] if batch["audio"] is not None and batch["audio"].numel() > 0 else None
                labels = batch["labels"][i:i+1] if batch["labels"] is not None and batch["labels"].numel() > 0 else None
                action_labels = batch["action_labels"][i:i+1] if batch["action_labels"] is not None and batch["action_labels"].numel() > 0 else None

                # Skip if no input modality is present or labels are missing for tasks requiring them
                if (input_ids is None and images is None and audio is None) or \
                   (task_type in ["text", "vision_text", "audio_text"] and labels is None):
                    continue

                # Forward pass
                outputs = self.model(
                    input_ids=input_ids,
                    images=images,
                    audio=audio,
                    task_type=task_type,
                    return_loss=True,
                    labels=labels,
                    action_labels=action_labels
                )

                # Check if loss is valid before accumulating
                if outputs["loss"] is not None and not torch.isnan(outputs["loss"]):
                    # Normalize loss by gradient accumulation steps
                    loss = outputs["loss"] / self.gradient_accumulation_steps
                    batch_loss += loss.item() # Accumulate item for display
                    loss.backward() # Accumulate gradients
                    valid_samples_in_batch += 1
                else:
                    logger.warning(f"Skipping sample {i} due to invalid loss (NaN or None) for task type: {task_type}")


                # Add load balancing loss from MoE layers
                # Normalize load balance loss by gradient accumulation steps and number of layers
                for block in self.model.transformer_blocks:
                    batch_load_balance_loss += block.moe.get_load_balancing_loss().item() / (self.gradient_accumulation_steps * len(self.model.transformer_blocks))  # type: ignore


            # Only proceed if there are valid samples in the batch
            if valid_samples_in_batch > 0:
                # Total batch loss for display (already normalized)
                current_batch_loss_display = batch_loss * self.gradient_accumulation_steps / valid_samples_in_batch if valid_samples_in_batch > 0 else 0.0
                current_load_balance_loss_display = batch_load_balance_loss * self.gradient_accumulation_steps if len(self.model.transformer_blocks) > 0 else 0.0

                # Perform optimization step only after accumulating gradients
                if (batch_idx + 1) % self.gradient_accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):
                     # Calculate gradient norm before clipping
                    grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    epoch_gradient_norms.append(grad_norm.item())

                    # Update parameters
                    self.optimizer.step()
                    self.optimizer.zero_grad() # Clear gradients after update

                    # Update scheduler (step per optimization step)
                    self.scheduler.step()

                # Update metrics (use the normalized batch loss)
                total_loss += current_batch_loss_display
                total_load_balance_loss += current_load_balance_loss_display


                # Update progress bar
                progress_bar.set_postfix({
                    "Loss": f"{current_batch_loss_display:.4f}",
                    "LB_Loss": f"{current_load_balance_loss_display:.4f}",
                    "Grad_Norm": f"{grad_norm.item():.4f}" if (batch_idx + 1) % self.gradient_accumulation_steps == 0 or (batch_idx + 1) == len(train_loader) else "N/A"
                })
            else:
                # If no valid samples, just update progress bar
                 progress_bar.set_postfix({
                    "Loss": "N/A",
                    "LB_Loss": "N/A",
                    "Grad_Norm": "N/A"
                })


        self.gradient_norm_history.append(np.mean(epoch_gradient_norms) if epoch_gradient_norms else 0.0) # Store average grad norm for epoch, handle empty case
        # The total loss accumulated over the epoch needs to be divided by the number of optimization steps
        num_optimization_steps = math.ceil(len(train_loader) / self.gradient_accumulation_steps)
        return total_loss / (num_optimization_steps if num_optimization_steps > 0 else 1) # Avoid division by zero


    def _apply_warmup_lr(self, step: int, warmup_steps: int):
        """Apply linear learning rate warmup"""
        if step < warmup_steps:
            warmup_factor = step / warmup_steps
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = self.config.learning_rate * warmup_factor

    def evaluate(self, val_loader: DataLoader) -> float:
        """Evaluate on validation set"""
        self.model.eval()
        total_loss = 0
        valid_batches = 0

        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Evaluating"):
                batch_loss = 0
                valid_samples_in_batch = 0

                for i, task_type in enumerate(batch["task_types"]):
                    input_ids = batch["input_ids"][i:i+1] if batch["input_ids"] is not None and batch["input_ids"].numel() > 0 else None
                    images = batch["images"][i:i+1] if batch["images"] is not None and batch["images"].numel() > 0 else None
                    audio = batch["audio"][i:i+1] if batch["audio"] is not None and batch["audio"].numel() > 0 else None
                    labels = batch["labels"][i:i+1] if batch["labels"] is not None and batch["labels"].numel() > 0 else None
                    action_labels = batch["action_labels"][i:i+1] if batch["action_labels"] is not None and batch["action_labels"].numel() > 0 else None

                    # Skip if no input modality is present or labels are missing for tasks requiring them
                    if (input_ids is None and images is None and audio is None) or \
                       (task_type in ["text", "vision_text", "audio_text"] and labels is None):
                        continue

                    outputs = self.model(
                        input_ids=input_ids,
                        images=images,
                        audio=audio,
                        task_type=task_type,
                        return_loss=True,
                        labels=labels,
                        action_labels=action_labels
                    )
                    # Check if loss is valid before accumulating
                    if outputs["loss"] is not None and not torch.isnan(outputs["loss"]):
                        batch_loss += outputs["loss"].item()
                        valid_samples_in_batch += 1
                    else:
                         logger.warning(f"Skipping sample {i} during evaluation due to invalid loss (NaN or None) for task type: {task_type}")


                if valid_samples_in_batch > 0:
                    total_loss += batch_loss / valid_samples_in_batch
                    valid_batches += 1

        return total_loss / (valid_batches if valid_batches > 0 else 1) # Avoid division by zero


    def train(self, train_loader: DataLoader, val_loader: DataLoader, num_epochs: int = 10):
        """Full training loop"""
        logger.info(f"Starting training for {num_epochs} epochs")

        # Calculate total training steps for warmup
        steps_per_epoch = math.ceil(len(train_loader) / self.gradient_accumulation_steps)
        self.total_training_steps = 0 # Reset step counter

        for epoch in range(num_epochs):
            logger.info(f"Epoch {epoch + 1}/{num_epochs}")

            # Train
            train_loss = self.train_epoch(train_loader)
            self.train_losses.append(train_loss)

            # Validate
            val_loss = self.evaluate(val_loader)
            self.val_losses.append(val_loss)

            # Log metrics
            logger.info(f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

            # Save expert usage statistics
            expert_usage = {}
            for i, block in enumerate(self.model.transformer_blocks):
                expert_usage[f"layer_{i}"] = block.moe.expert_usage.clone()  # type: ignore
            self.expert_usage_history.append(expert_usage)

            # Plot training progress and save to disk
            if (epoch + 1) % 1 == 0: # Plot every epoch for better monitoring
                self.plot_training_progress(save_dir="./SalesA")

        logger.info("Training finished.")
        # After training, plot and save bias/diagnostic visualizations
        self.plot_bias_and_diagnostics(val_loader, save_dir="./SalesA")

    def plot_training_progress(self, save_dir: str = "./SalesA"):
        """Plot training and validation losses, gradient norms, and expert usage, and save to disk."""
        num_plots = 3 if self.expert_usage_history and self.gradient_norm_history else (2 if self.gradient_norm_history else (1 if self.expert_usage_history else 1))
        fig, axes = plt.subplots(1, num_plots, figsize=(6 * num_plots, 4))
        axes = axes.flatten() # Ensure axes is iterable even for a single subplot

        plot_idx = 0

        # Loss curves
        axes[plot_idx].plot(self.train_losses, label='Training Loss', color='blue')
        axes[plot_idx].plot(self.val_losses, label='Validation Loss', color='red')
        axes[plot_idx].set_title('Training Progress')
        axes[plot_idx].set_xlabel('Epoch')
        axes[plot_idx].set_ylabel('Loss')
        axes[plot_idx].legend()
        axes[plot_idx].grid(True)
        plot_idx += 1

        # Gradient Norms
        if self.gradient_norm_history:
            axes[plot_idx].plot(self.gradient_norm_history, label='Avg Gradient Norm', color='green')
            axes[plot_idx].set_title('Average Gradient Norm per Epoch')
            axes[plot_idx].set_xlabel('Epoch')
            axes[plot_idx].set_ylabel('Gradient Norm')
            axes[plot_idx].legend()
            axes[plot_idx].grid(True)
            plot_idx += 1

        # Expert usage distribution
        if self.expert_usage_history:
            latest_usage = self.expert_usage_history[-1]
            # Assuming we plot usage for the first MoE layer
            if "layer_0" in latest_usage:
                layer_0_usage = latest_usage["layer_0"].cpu().numpy()
                axes[plot_idx].bar(range(len(layer_0_usage)), layer_0_usage)
                axes[plot_idx].set_title('Expert Usage Distribution (Layer 0)')
                axes[plot_idx].set_xlabel('Expert ID')
                axes[plot_idx].set_ylabel('Usage Count')
                axes[plot_idx].grid(True)
                plot_idx += 1

        plt.tight_layout()
        # Always save the plot
        os.makedirs(save_dir, exist_ok=True)
        plt.savefig(os.path.join(save_dir, f"training_progress_epoch_{len(self.train_losses)}.png"))
        plt.close()

    def save_model(self, folder_path: str = "./SalesA"):
        """Save the trained model and associated files for deployment."""
        # Create the directory if it doesn't exist
        os.makedirs(folder_path, exist_ok=True)
        logger.info(f"Saving model files to: {folder_path}")

        # Save model state dictionary in safetensors format
        model_path = os.path.join(folder_path, "model.safetensors")
        save_file(self.model.state_dict(), model_path)
        logger.info(f"Model weights saved to {model_path}")

        # Save configuration
        config_path = os.path.join(folder_path, "config.json")
        with open(config_path, "w") as f:
            json.dump(self.config.__dict__, f, indent=4)
        logger.info(f"Configuration saved to {config_path}")

        # Save training history
        history_path = os.path.join(folder_path, "training_history.pkl")
        history_data = {
            "train_losses": self.train_losses,
            "val_losses": self.val_losses,
            "expert_usage_history": self.expert_usage_history,
            "gradient_norm_history": self.gradient_norm_history # Save gradient norm history
        }
        with open(history_path, "wb") as f:
            pickle.dump(history_data, f)
        logger.info(f"Training history saved to {history_path}")

        # Save Tiktoken vocab with author info
        tokenizer_vocab_path = os.path.join(folder_path, "vocab.json")
        try:
            vocab_data = {
                "model_author": self.model.get_author(),
                "vocab_size": self.model.text_encoder.embedding.weight.shape[0]
            }
            with open(tokenizer_vocab_path, "w") as f:
                json.dump(vocab_data, f, indent=4)
            logger.info(f"Tokenizer vocabulary saved to {tokenizer_vocab_path}")
        except Exception as e:
            logger.error(f"Could not save tokenizer vocabulary: {e}")
        # Save model architecture summary
        model_summary_path = os.path.join(folder_path, "model_summary.txt")
        with open(model_summary_path, "w") as f:
            f.write(f"Model Name: {self.model.get_name()}\n")
            f.write(f"Author: {self.model.get_author()}\n\n")
            f.write(str(self.model))
        logger.info(f"Model summary saved to {model_summary_path}")
        # Only generate merge.txt and tokenizer.model if using subword tokenizer
        # (Assume not used here, so skip or document in README)
        readme_path = os.path.join(folder_path, "README.md")
        with open(readme_path, "a") as f:
            f.write(f"\nCreated by: {self.model.get_author()}\n")
            f.write("\nNote: merge.txt and tokenizer.model are not generated as this model uses a Tiktoken-based tokenizer.\n")
        # Improve chat_template.jinja
        chat_template_path = os.path.join(folder_path, "chat_template.jinja")
        with open(chat_template_path, "w") as f:
            f.write("""{# Jinja2 template for chat formatting #}\n{{ user }}: {{ message }}\n---\n{{ assistant }}: {{ response }}\n""")
        logger.info(f"chat_template.jinja saved to {chat_template_path}")

        # Save tokenizer.json with author info
        tokenizer_json = {
            "vocab": self.model.text_encoder.embedding.weight.shape[0],
            "model_author": self.model.get_author(),
            "pad_token": self.model.config.model_name if hasattr(self.model.config, 'model_name') else None
        }
        tokenizer_path = os.path.join(folder_path, "tokenizer.json")
        with open(tokenizer_path, "w") as f:
            json.dump(tokenizer_json, f, indent=2)
        logger.info(f"Tokenizer config saved to {tokenizer_path}")

        logger.info("Model and associated files saved successfully for deployment.")


    def load_model(self, filepath: str):
        """Load a pre-trained model"""
        checkpoint = torch.load(filepath, map_location='cpu')
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.train_losses = checkpoint['train_losses']
        self.val_losses = checkpoint['val_losses']
        self.expert_usage_history = checkpoint['expert_usage_history']
        if 'gradient_norm_history' in checkpoint: # Load gradient norm history if exists
             self.gradient_norm_history = checkpoint['gradient_norm_history']
        logger.info(f"Model loaded from {filepath}")

    def plot_bias_and_diagnostics(self, val_loader, save_dir="./SalesA"):
        """Plot and save bias/diagnostic visualizations after training."""
        all_true = []
        all_pred = []
        self.model.eval()
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch["input_ids"]
                labels = batch["labels"]
                task_types = batch["task_types"]
                for i, task_type in enumerate(task_types):
                    # Only process classification tasks (e.g., financial sentiment)
                    if task_type == "text" and labels[i].numel() == 1:
                        outputs = self.model(input_ids=input_ids[i:i+1])
                        pred = torch.argmax(outputs["logits"], dim=-1).item()
                        all_pred.append(pred)
                        all_true.append(labels[i].item())
        if all_true and all_pred:
            # Confusion matrix
            cm = confusion_matrix(all_true, all_pred)
            plt.figure(figsize=(6,6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.title("Confusion Matrix")
            plt.xlabel("Predicted")
            plt.ylabel("True")
            os.makedirs(save_dir, exist_ok=True)
            plt.savefig(os.path.join(save_dir, "confusion_matrix.png"))
            plt.close()

            # Class distribution
            plt.figure(figsize=(8,4))
            sns.histplot(all_true, color='blue', label='True', kde=False, bins=len(set(all_true)), alpha=0.5)
            sns.histplot(all_pred, color='red', label='Predicted', kde=False, bins=len(set(all_pred)), alpha=0.5)
            plt.legend()
            plt.title("Class Distribution")
            plt.savefig(os.path.join(save_dir, "class_distribution.png"))
            plt.close()

            # Per-class metrics
            report = classification_report(all_true, all_pred, output_dict=True)
            plt.figure(figsize=(8,4))
            if isinstance(report, dict):
                class_indices = sorted([int(cls) for cls in report.keys() if str(cls).isdigit()])
                for metric in ["precision", "recall", "f1-score"]:
                    values = [report[str(cls)][metric] for cls in class_indices]
                    plt.plot(class_indices, values, label=metric)
            plt.legend()
            plt.title("Per-class Metrics")
            plt.savefig(os.path.join(save_dir, "per_class_metrics.png"))
            plt.close()

"""# 9. EVALUATION METRICS"""

class SalesAEvaluator:
    """Evaluation class for SalesA AI"""
    def __init__(self, model: SalesAModel, tokenizer: SalesATokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def evaluate_text_generation(self, prompts: List[str], max_length: int = 100) -> Dict:
        """Evaluate text generation capability"""
        self.model.eval()
        results = []

        with torch.no_grad():
            for prompt in prompts:
                # Encode prompt
                input_ids = torch.tensor([self.tokenizer.encode(prompt)])

                # Generate
                generated_ids = self.model.generate(
                    input_ids,
                    max_length=max_length,
                    temperature=0.7,
                    do_sample=True,
                    top_k=50
                )

                # Decode
                generated_text = self.tokenizer.decode(generated_ids[0].tolist())
                results.append({
                    "prompt": prompt,
                    "generated": generated_text,
                    "length": len(generated_ids[0])
                })

        return {
            "results": results,
            "avg_length": np.mean([r["length"] for r in results])
        }

    def evaluate_code_generation(self, code_prompts: List[str]) -> Dict:
        """Evaluate code generation capability"""
        self.model.eval()
        results = []

        with torch.no_grad():
            for prompt in code_prompts:
                # Add code token
                full_prompt = f"<CODE>{prompt}"
                input_ids = torch.tensor([self.tokenizer.encode(full_prompt)])

                # Generate code
                generated_ids = self.model.generate(
                    input_ids,
                    max_length=200,
                    temperature=0.3,  # Lower temperature for code
                    do_sample=True,
                    top_k=20
                )

                generated_code = self.tokenizer.decode(generated_ids[0].tolist())
                results.append({
                    "prompt": prompt,
                    "generated_code": generated_code,
                    "syntax_valid": self._check_python_syntax(generated_code)
                })

        return {
            "results": results,
            "syntax_accuracy": np.mean([r["syntax_valid"] for r in results])
        }

    def _check_python_syntax(self, code: str) -> bool:
        """Check if generated code has valid Python syntax"""
        try:
            compile(code, "<string>", "exec")
            return True
        except SyntaxError:
            return False

    def evaluate_multimodal_understanding(self, test_cases: List[Dict]) -> Dict:
        """Evaluate multimodal understanding"""
        self.model.eval()
        results = []

        with torch.no_grad():
            for case in test_cases:
                # Prepare inputs
                input_ids = None
                images = None
                audio = None

                if "text" in case:
                    input_ids = torch.tensor([self.tokenizer.encode(case["text"])])

                if "image" in case:
                    images = case["image"].unsqueeze(0)

                if "audio" in case:
                    audio = case["audio"].unsqueeze(0)

                # Forward pass
                outputs = self.model(
                    input_ids=input_ids,
                    images=images,
                    audio=audio,
                    task_type=case["task_type"]
                )

                # Get predictions
                predictions = torch.argmax(outputs["logits"], dim=-1)
                predicted_text = self.tokenizer.decode(predictions[0].tolist())

                results.append({
                    "case": case,
                    "prediction": predicted_text,
                    "confidence": torch.max(F.softmax(outputs["logits"], dim=-1)).item()
                })

        return {
            "results": results,
            "avg_confidence": np.mean([r["confidence"] for r in results])
        }

    def analyze_expert_usage(self) -> Dict:
        """Analyze MoE expert usage patterns"""
        expert_stats = {}

        for i, block in enumerate(self.model.transformer_blocks):
            usage = block.moe.expert_usage.cpu().numpy()  # type: ignore
            expert_stats[f"layer_{i}"] = {
                "usage_counts": usage.tolist(),
                "most_used_expert": int(np.argmax(usage)),
                "least_used_expert": int(np.argmin(usage)),
                "usage_variance": float(np.var(usage)),
                "load_balance_score": float(np.std(usage) / (np.mean(usage) + 1e-8))
            }

        return expert_stats

"""# 10. MAIN EXECUTION AND COLAB INTEGRATION"""

def main():
    """Main execution function for SalesA AI"""
    print("=" * 80)
    # Use model name and author from config
    config = SalesAConfig(
        vocab_size=32000,
        hidden_dim=512,
        num_layers=8,
        num_heads=8,
        num_experts=4,
        batch_size=4,
    )
    print(f"{config.model_name} - Complete Implementation")
    print(f"{config.model_author}")
    print("=" * 80)

    # Initialize components
    print("\n1. Initializing SalesA AI components...")
    # Load raw training data for vocab building
    raw_train_dataset = MultimodalDataset(config, SalesATokenizer(vocab_size=config.vocab_size), split="train")
    vocab, enc = build_vocab_with_tiktoken(raw_train_dataset.data, vocab_size=config.vocab_size, model_name="gpt2")
    tokenizer = SalesATokenizer(vocab_size=config.vocab_size, vocab=vocab, enc=enc)
    trainer = SalesATrainer(config)
    evaluator = SalesAEvaluator(trainer.model, tokenizer)

    # Model summary
    total_params = sum(p.numel() for p in trainer.model.parameters())
    trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")

    # Create datasets
    print("\n2. Creating datasets...")
    # Now create datasets with the real tokenizer
    train_dataset = MultimodalDataset(config, tokenizer, split="train")
    val_dataset = MultimodalDataset(config, tokenizer, split="val")

    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0  # Set to 0 for CPU
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0
    )

    print(f"Training samples: {len(train_dataset)}")
    print(f"Validation samples: {len(val_dataset)}")

    # Training
    print("\n3. Starting training...")
    trainer.train(train_loader, val_loader, num_epochs=5)  # Reduced epochs for demo

    # Evaluation
    print("\n4. Evaluating model...")

    # Text generation evaluation
    text_prompts = [
        "The future of AI is",
        "In a world where technology",
        "Machine learning algorithms"
    ]

    text_results = evaluator.evaluate_text_generation(text_prompts)
    print(f"Text generation - Average length: {text_results['avg_length']:.2f}")

    # Code generation evaluation
    code_prompts = [
        "Write a function to calculate fibonacci numbers",
        "Create a class for a binary tree",
        "Implement quicksort algorithm"
    ]

    code_results = evaluator.evaluate_code_generation(code_prompts)
    print(f"Code generation - Syntax accuracy: {code_results['syntax_accuracy']:.2f}")

    # Expert usage analysis
    expert_stats = evaluator.analyze_expert_usage()
    print(f"Expert usage analysis completed for {len(expert_stats)} layers")

    # Save model
    print("\n5. Saving model...")
    trainer.save_model("./SalesA")

    # Demo inference
    print("\n6. Demo inference...")
    demo_text = "Hello, I am SalesA AI"
    input_ids = torch.tensor([tokenizer.encode(demo_text)])

    with torch.no_grad():
        outputs = trainer.model(input_ids, task_type="text")
        next_token_probs = F.softmax(outputs["logits"][0, -1, :], dim=-1)
        top_tokens = torch.topk(next_token_probs, 5)

        print(f"Input: {demo_text}")
        print("Top 5 next token predictions:")
        for i, (prob, token_id) in enumerate(zip(top_tokens.values, top_tokens.indices)):
            token = tokenizer.id_to_token[int(token_id.item())]
            print(f"  {i+1}. {token} (prob: {prob.item():.4f})")

    print("\n" + "=" * 80)
    print("SalesA AI training and evaluation completed successfully!")
    print("The model demonstrates:")
    print("✓ Multimodal processing (text, vision, audio)")
    print("✓ Mixture of Experts architecture")
    print("✓ CPU-optimized training")
    print("✓ Code generation capabilities")
    print("✓ Human-like text generation")
    print("=" * 80)

"""# 12. ADDITIONAL UTILITIES AND HELPERS"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional, Union
import json
import os
from pathlib import Path
import logging
from dataclasses import dataclass
import math
import random
from collections import OrderedDict
import torchvision.transforms as transforms
from PIL import Image
import torchaudio
import re
import pickle
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
from safetensors.torch import save_file # Import save_file

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def export_for_deployment(model: SalesAModel, config: SalesAConfig, tokenizer: SalesATokenizer, export_dir: str = "./SalesA"):
    """Export model and assets for deployment (Hugging Face compatible)"""
    os.makedirs(export_dir, exist_ok=True)

    # 1. Save model weights in safetensors format
    model_path = os.path.join(export_dir, "model.safetensors")
    save_file(model.state_dict(), model_path)

    # 2. Save config as JSON
    config_path = os.path.join(export_dir, "config.json")
    with open(config_path, "w") as f:
        json.dump(config.__dict__, f, indent=2)

    # 3. Save tokenizer files
    tokenizer_json = {
        "vocab": tokenizer.vocab,
        "token_to_id": tokenizer.token_to_id,
        "id_to_token": tokenizer.id_to_token,
        "pad_token": tokenizer.pad_token,
        "unk_token": tokenizer.unk_token,
        "bos_token": tokenizer.bos_token,
        "eos_token": tokenizer.eos_token,
        "code_token": tokenizer.code_token,
        "pad_token_id": tokenizer.pad_token_id,
        "unk_token_id": tokenizer.unk_token_id,
        "bos_token_id": tokenizer.bos_token_id,
        "eos_token_id": tokenizer.eos_token_id,
        "code_token_id": tokenizer.code_token_id,
        "model_author": config.model_author
    }
    tokenizer_path = os.path.join(export_dir, "tokenizer.json")
    with open(tokenizer_path, "w") as f:
        json.dump(tokenizer_json, f, indent=2)

    # 4. Save training arguments/metadata
    training_args = {
        "learning_rate": config.learning_rate,
        "weight_decay": config.weight_decay,
        "dropout_rate": config.dropout_rate,
        "batch_size": config.batch_size,
        "num_epochs": 10 # Updated epochs to match main execution
        # Add other relevant training parameters here
    }
    training_args_path = os.path.join(export_dir, "training_args.json")
    with open(training_args_path, "w") as f:
        json.dump(training_args, f, indent=2)
    print(f"✓ Training arguments saved to {training_args_path}")

    # --- Additional required files ---
    # 1. merge.txt (for tokenizer merges, placeholder if not used)
    merge_path = os.path.join(export_dir, "merge.txt")
    with open(merge_path, "w") as f:
        f.write("# Placeholder for tokenizer merges (if using BPE or similar)\n")
    # 2. README.md
    readme_path = os.path.join(export_dir, "README.md")
    with open(readme_path, "w") as f:
        f.write(f"Created by: {config.model_author}\n\n")
        f.write("""# SalesA AI Model\n\nThis directory contains the SalesA AI multimodal model, tokenizer, and all associated files for deployment and inference.\n\n- Model weights: model.safetensors\n- Config: config.json\n- Tokenizer: tokenizer.json, vocab.json, tokenizer.model\n- Generation config: generation_config.json\n- Merge file: merge.txt\n- Index: model.safetensors.index.json\n- Chat template: chat_template.jinja\n\nFor more details, see the documentation.\n""")
    # 3. chat_template.jinja (template for chat UIs, placeholder)
    chat_template_path = os.path.join(export_dir, "chat_template.jinja")
    with open(chat_template_path, "w") as f:
        f.write("""{# Jinja2 template for chat formatting #}\n{{ user }}: {{ message }}\n""")
    # 4. generation_config.json (default generation parameters)
    generation_config_path = os.path.join(export_dir, "generation_config.json")
    generation_config = {
        "max_length": 128,
        "temperature": 0.7,
        "top_k": 50,
        "do_sample": True
    }
    with open(generation_config_path, "w") as f:
        json.dump(generation_config, f, indent=2)
    # 5. model.safetensors.index.json (index file for sharded weights, placeholder)
    index_path = os.path.join(export_dir, "model.safetensors.index.json")
    with open(index_path, "w") as f:
        json.dump({"weight_map": {"model.safetensors": []}}, f, indent=2)
    # 6. tokenizer.model (placeholder for SentencePiece or similar)
    tokenizer_model_path = os.path.join(export_dir, "tokenizer.model")
    with open(tokenizer_model_path, "wb") as f:
        f.write(b"# Placeholder for tokenizer.model (e.g., SentencePiece)")
    print(f"✓ Model exported to {export_dir} (model.safetensors, config.json, tokenizer.json, training_args.json, merge.txt, README.md, chat_template.jinja, generation_config.json, model.safetensors.index.json, tokenizer.model)")

"""# 13. EXECUTION ENTRY **POINT**"""

if __name__ == "__main__":
    main()  # Call the original main function for standard environment

# --- NEW: Utility to build vocab from training data using Tiktoken ---
def build_vocab_with_tiktoken(dataset, vocab_size=32000, model_name="gpt2"):
    # Use Tiktoken's BPE model (e.g., gpt2) to build vocab from data
    enc = tiktoken.get_encoding(model_name)
    counter = {}
    for sample in dataset:
        text = sample.get("text")
        if text is not None:
            if isinstance(text, torch.Tensor):
                text_str = " ".join([str(t.item()) for t in text])
            elif isinstance(text, str):
                text_str = text
            else:
                continue
            tokens = enc.encode(text_str)
            for token in tokens:
                counter[token] = counter.get(token, 0) + 1
    # Get most common tokens up to vocab_size minus extra tokens
    most_common = sorted(counter.items(), key=lambda x: -x[1])[:max(0, vocab_size - len(EXTRA_TOKENS))]
    vocab = EXTRA_TOKENS + [token for token, _ in most_common]
    return vocab[:vocab_size], enc
