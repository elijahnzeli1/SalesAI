# Multimodal training configuration
extends: base.yaml

model:
  name: "SalesA AI - Multimodal"
  num_layers: 12
  hidden_dim: 1024
  num_experts: 16  # More experts for modality-specific processing
  
  # Enhanced multimodal dimensions
  vision_dim: 384  # Larger vision patches
  audio_dim: 128  # Higher audio resolution
  vision_patch_size: 14
  audio_patch_size: 2

training:
  batch_size: 2  # Smaller batch due to multiple modalities
  learning_rate: 1.0e-4
  num_epochs: 25
  gradient_accumulation_steps: 8  # Effective batch size of 16
  use_mixed_precision: true  # Enable AMP for GPU training

data:
  dataset_name: ["luma", "clotho_aqa", "beans"]  # Mix of modalities
  task_type: "multimodal"
  max_text_length: 1024
  max_audio_length: 32000  # 2 seconds at 16kHz
  num_workers: 4  # More workers for multimodal processing
  
  # Additional multimodal settings
  vision_augment: true
  audio_augment: true
  text_augment: true 